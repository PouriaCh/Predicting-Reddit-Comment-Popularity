{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset \n",
    "current_folder = os.getcwd()\n",
    "dataset_filename = \"comments.json\"\n",
    "with open(os.path.join(current_folder, \"data\", dataset_filename)) as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now the data is loaded.\n",
    "It a list of data points, where each datapoint is a dictionary with the following attributes:\n",
    "$ popularity_score : a popularity score for this comment (based on the number of upvotes) (type: float)\n",
    "$ children : the number of replies to this comment (type: int)\n",
    "$ text : the text of this comment (type: string)\n",
    "$ controversiality : a score for how \"controversial\" this comment is (automatically computed by Reddit)\n",
    "$ is_root : if True, then this comment is a direct reply to a post; if False, this is a direct reply to another comment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "data_point = data[1100] # select the first data point in the dataset\n",
    "\n",
    "# Now we print all the information about this datapoint\n",
    "for name, value in data_point.items():\n",
    "    print(name + \" : \" + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into partitions\n",
    "\n",
    "no_data_point = 12000\n",
    "training_data_points = 10000\n",
    "validation_data_points = 1000\n",
    "test_data_points = 1000\n",
    "\n",
    "train_data = data[ : training_data_points]\n",
    "validation_data = data[training_data_points : (training_data_points + validation_data_points)]\n",
    "test_data = data[(training_data_points + validation_data_points) : no_data_point]\n",
    "\n",
    "print(\"Number of training data points: \" + str(len(train_data)))\n",
    "print(\"Number of evaluation data points: \" + str(len(validation_data)))\n",
    "print(\"Number of test data points: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complementary Functions\n",
    "\n",
    "# Weight vector initialization\n",
    "def weights_init(m, key):\n",
    "    # key = True => zero initialization\n",
    "    # key = False => Random initialization\n",
    "    if key:\n",
    "        w = np.zeros((m,1))\n",
    "    else:\n",
    "        w = np.random.randi(m,1)\n",
    "    \n",
    "    return w\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Text pre-processing \n",
    "\n",
    "def text_prep(text):\n",
    "    punctuations_list = '!-[]{};\\,<>/\"?#$%^&*_~+' + '\\n\\n'\n",
    "    output_text = \"\"\n",
    "    text = text.strip()\n",
    "    \n",
    "    for ch in text:\n",
    "        if ch not in punctuations_list:\n",
    "            output_text = output_text + ch\n",
    "    \n",
    "    output_text = output_text.lower()\n",
    "    prep_text = output_text.split(\" \")\n",
    "    prep_text = list(filter(None,prep_text))\n",
    "    \n",
    "    return prep_text\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Most-frequent word count features for the whole Training set\n",
    "def Feature_Matrix(dataset, no_txt_features):\n",
    "    \n",
    "    offensive_words_list=['shit','fuck','fucking','bitch','damn','sex','ass','hell',\n",
    "         'hot','dick','shitty','fucked','asshole','bullshit','gay','porn','crap','sucks']\n",
    "    \n",
    "    positive_sentiments_list = ['like', 'really', 'good', 'please', 'love', 'pretty', 'best', \n",
    "                                'better', 'great', 'movie', 'happy', 'watching', 'nice', 'fun', \n",
    "                                'thanks', 'thank', 'funny', 'cool', 'thankfully', 'super', 'enjoy', \n",
    "                                'awesome' , 'wow', 'amazing', 'interesting', 'loved’, ‘liked', 'perfect',\n",
    "                                'fan', 'glad', 'haha', 'fans', 'hilarious', 'popular', 'fair', 'special', \n",
    "                                'beautiful', ':d', ':)', '=d']\n",
    "    \n",
    "    negative_sentiments_list = ['bad', 'hate', 'wrong', 'lost', 'damn', 'hell', 'sorry', 'dead', 'weird', 'shitty',\n",
    "                                'worst', 'terrible', 'worse', 'sad', 'seeing', 'die', 'death', 'died', 'kill', 'poor',\n",
    "                                'breaking', 'horrible', ':(', '=(']\n",
    "    \n",
    "    N = len(dataset)\n",
    "    All_comments = [None] * N\n",
    "    Words_Dict = {}\n",
    "    bias = np.ones((N,1))\n",
    "    controversiality_vec = np.zeros((N,1))\n",
    "    is_root_vec = np.zeros((N,1))\n",
    "    children_vec = np.zeros((N,1))\n",
    "    Y = np.zeros((N,1))\n",
    "    X_words_count = np.zeros((N,no_txt_features))\n",
    "    offensive_count = np.zeros((N,1))\n",
    "    http_count = np.zeros((N,1))\n",
    "    positive_sentiments = np.zeros((N,1))\n",
    "    negative_sentiments = np.zeros((N,1))\n",
    "    \n",
    "    for i in range(N):\n",
    "        for key,val in dataset[i].items():\n",
    "            if key == 'text':\n",
    "                preprocessed_text = text_prep(val)\n",
    "                All_comments[i] = preprocessed_text\n",
    "                for x in preprocessed_text:\n",
    "                    if x not in Words_Dict.keys():\n",
    "                        Words_Dict[x] = 0\n",
    "                    Words_Dict[x] += 1\n",
    "            \n",
    "            elif key == 'is_root':\n",
    "                if val:\n",
    "                    is_root_vec[i] = 1\n",
    "                else:\n",
    "                    is_root_vec[i] = 0\n",
    "\n",
    "            elif key == 'controversiality':\n",
    "                controversiality_vec[i] = val\n",
    "            \n",
    "            elif key == 'children':\n",
    "                children_vec[i] = val\n",
    "            \n",
    "            elif key == 'popularity_score':\n",
    "                Y[i] = val\n",
    "\n",
    "# Now we need to sort out from most-frequent to least-frequent words in dictionary to obtain the first N words\n",
    "    if no_txt_features == 0 :\n",
    "        \n",
    "        X1 = np.append(children_vec,controversiality_vec,axis=1)\n",
    "        X1 = np.append(X1,is_root_vec,axis=1)        \n",
    "        X = np.append(X1,bias,axis=1)\n",
    "        \n",
    "    else:\n",
    "        Words_Dict_Sorted = sorted(Words_Dict.items(), key = lambda t: t[1], reverse=True)\n",
    "        Most_Freq_Words_Dict = dict(list(Words_Dict_Sorted[:no_txt_features]))\n",
    "        Most_Freq_Words = list(Most_Freq_Words_Dict.keys())\n",
    "        \n",
    "# Now we need to count the frequency of most frequent words in each comment throughout the whole dataset\n",
    "                \n",
    "        # Most-Frequent words\n",
    "        for i in range(N):\n",
    "            for j in range(no_txt_features):  \n",
    "                X_words_count[i,j] = All_comments[i].count(Most_Freq_Words[j])     \n",
    "        \n",
    "        # Offensive and HTTP-containing comments\n",
    "        \n",
    "        for i in range(N):\n",
    "            for x in All_comments[i]:\n",
    "                if x.find('http') or x.find('www.') != -1 :\n",
    "                    http_count[i] = 1\n",
    "                \n",
    "                for l in range(len(positive_sentiments_list)):\n",
    "                    if x.find(positive_sentiments_list[l]) != -1 :\n",
    "                        positive_sentiments[i] += 1\n",
    "                \n",
    "                for l in range(len(negative_sentiments_list)):\n",
    "                    if x.find(negative_sentiments_list[l]) != -1 :\n",
    "                        negative_sentiments[i] += 1\n",
    "                        \n",
    "            for j in range(len(offensive_words_list)):  \n",
    "                offensive_count[i] += All_comments[i].count(offensive_words_list[j])\n",
    "            \n",
    "        for j in range(len(offensive_words_list)):  \n",
    "            if offensive_count[i] > 0:\n",
    "                offensive_count[i] = 1\n",
    "        \n",
    "        \n",
    "        # Positive or negative sentiments\n",
    "        \n",
    "        X1 = np.append(X_words_count,controversiality_vec,axis=1)\n",
    "        X1 = np.append(X1,is_root_vec,axis=1)\n",
    "        X1 = np.append(X1,children_vec,axis=1)\n",
    "        X1 = np.append(X1,offensive_count,axis=1)\n",
    "        X1 = np.append(X1,http_count,axis=1)\n",
    "        X1 = np.append(X1,positive_sentiments,axis=1)\n",
    "        X1 = np.append(X1,negative_sentiments,axis=1)\n",
    "        \n",
    "        # Data Rescaling\n",
    "        \n",
    "        for i in range(X1.shape[1]):\n",
    "            mean = np.mean(X1[:,i])\n",
    "            var = np.var(X1[:,i])\n",
    "            X1[:,i] = (1 / np.sqrt(var)) * (X1[:,i] - mean)\n",
    "        \n",
    "        X = np.append(X1,bias,axis=1)       \n",
    "        \n",
    "    return X, Y, Most_Freq_Words_Dict\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Visualisations\n",
    "def lineplot(x_data, y_data, x_label=\"\", y_label=\"\", title=\"\", gcolor=\"\"):\n",
    "    # Create the plot object    \n",
    "    plt.figure()\n",
    "    # Plot the best fit line, set the linewidth (lw), color and\n",
    "    # transparency (alpha) of the line\n",
    "    plt.plot(x_data, y_data, lw = 2, color = gcolor, alpha = 1)\n",
    "    \n",
    "    # Label the axes and provide a title\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# MSE\n",
    "def Mean_Square_Error(X, Y, w_hat):\n",
    "    prediction = np.dot(X,w_hat)\n",
    "    abs_err = np.subtract(Y,prediction)\n",
    "    squared_err = np.square(abs_err)\n",
    "    MSE = (1/X.shape[0]) * np.sum(squared_err)  \n",
    "    return MSE\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Gradient Descent\n",
    "def Gradient_Descent(X, Y, W0, B, E, eps):\n",
    "    \n",
    "    eta0 = E\n",
    "    beta = B\n",
    "    epsilon = eps\n",
    "    w0 = W0\n",
    "    mse = np.zeros(100000000) # some big number for number of epochs\n",
    "    X_T = np.dot(X.T, X)\n",
    "    X_Y = np.dot(X.T, Y)\n",
    "    alpha = eta0 / ( 1 + beta )\n",
    "    w_gd = w0 - 2 * alpha * (np.subtract(np.dot(X_T,w0), X_Y))\n",
    "    diff = np.linalg.norm(np.subtract(w_gd,w0))\n",
    "    epoch = 0\n",
    "    mse[epoch] = Mean_Square_Error(X, Y, w_gd)\n",
    "    \n",
    "    while diff > epsilon:\n",
    "        w0 = w_gd\n",
    "        alpha =  eta0 / (1 + beta * (epoch + 1))\n",
    "        w_gd = w0 - 2 * alpha * np.subtract((X_T).dot(w0), X_Y)\n",
    "        diff = np.linalg.norm(np.subtract(w_gd, w0))\n",
    "        epoch +=1\n",
    "        mse[epoch] = Mean_Square_Error(X, Y, w_gd)\n",
    "    \n",
    "    MSE = np.delete(mse, np.s_[epoch + 1 : len(mse) + 1]) # Removing zero-valued MSE at the end\n",
    "    \n",
    "    return w_gd, MSE\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Least-Square estimation\n",
    "\n",
    "def Least_Squares_Estimation(X,Y):\n",
    "    X_T = (X.T).dot(X)\n",
    "    X_T_inv = np.linalg.inv(X_T)\n",
    "    X_Y = (X.T).dot(Y)\n",
    "    w_hat = np.dot(X_T_inv, X_Y)\n",
    "\n",
    "    return w_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation on training/validation\n",
    "\n",
    "no_text_features = 160\n",
    "Eta0 = 1e-6\n",
    "Beta0 = 0.0001\n",
    "epsilon = 1e-6\n",
    "X,Y,most_freq_words = Feature_Matrix(train_data, no_text_features)\n",
    "W0 = weights_init(X.shape[1], True) # All_zero weights initialization if True\n",
    "\n",
    "start1 = time.time()\n",
    "W_LS = Least_Squares_Estimation(X, Y)\n",
    "MSE_LS_training = Mean_Square_Error(X, Y, W_LS)\n",
    "end1 = time.time()\n",
    "\n",
    "print(\"MSE for LS on Training set is: \" + str(MSE_LS_training) + \" which took: \" + str((end1-start1) * 1000) + \" ms\" + \"\")\n",
    "\n",
    "start2 = time.time()\n",
    "W_GD, MSE_GD_training = Gradient_Descent(X, Y, W0, Beta0, Eta0, epsilon)\n",
    "end2 = time.time()\n",
    "print(\"Final MSE for GD on Training set at \" + str(len(MSE_GD_training)) + \"'s epoch is :\" + str(MSE_GD_training[-1]) +\n",
    "     \" which took: \" + str((end2-start2) * 1000) + \" ms\" + \"\")\n",
    "\n",
    "\n",
    "# Run on Validation dataset\n",
    "\n",
    "X_validation, Y_validation, _ = Feature_Matrix(validation_data, no_text_features)\n",
    "\n",
    "MSE_LS_validation = Mean_Square_Error(X_validation, Y_validation, W_LS)\n",
    "\n",
    "print(\"MSE for LS on Validation set is: \" + str(MSE_LS_validation))\n",
    "\n",
    "MSE_GD_validation = Mean_Square_Error(X_validation, Y_validation, W_GD)\n",
    "\n",
    "print(\"Final MSE for GD on Validation set is :\" + str(MSE_GD_validation))\n",
    "\n",
    "# Plots \n",
    "# training set\n",
    "lineplot(range(MSE_GD_training.shape[0]), MSE_GD_training, \n",
    "         x_label=\"epochs\", y_label=\"MSE\", title=\"Gradient Descent for Training set\", gcolor='b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P=erformance evaluation on test\n",
    "\n",
    "X_test, Y_test,_ = Feature_Matrix(test_data, no_text_features)\n",
    "\n",
    "MSE_LS_Test = Mean_Square_Error(X_test, Y_test, W_LS)\n",
    "\n",
    "print(\"MSE for LS on Test set is: \" + str(MSE_LS_Test))\n",
    "\n",
    "MSE_GD_validation = Mean_Square_Error(X_test, Y_test, W_GD)\n",
    "\n",
    "print(\"MSE for GD on Test set is :\" + str(MSE_GD_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (caffe)",
   "language": "python",
   "name": "op"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
